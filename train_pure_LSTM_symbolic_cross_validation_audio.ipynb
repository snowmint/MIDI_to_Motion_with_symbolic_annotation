{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66dc6d36-fc6d-4916-b7b0-6a2dfaf9c4fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from model import Encoder, Decoder, Seq2Seq\n",
    "from data_loader import *\n",
    "import pandas as pd\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "import pretty_midi\n",
    "import glob\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fd1d2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "import math\n",
    "matplotlib.use('Agg')\n",
    "# matplotlib.use(\"QtAgg\")\n",
    "import ffmpeg\n",
    "#conda install -c conda-forge ffmpeg-python\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, writers\n",
    "plt.rcParams['animation.ffmpeg_path'] = '/home/ilc/anaconda3/bin/ffmpeg'#'/usr/bin/ffmpeg'\n",
    "\n",
    "import numpy as np\n",
    "import subprocess as sp\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "from moviepy.audio.io.AudioFileClip import AudioFileClip\n",
    "\n",
    "from midi2audio import FluidSynth\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b387469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b26d6b8a-e8ec-4fa2-b14f-a45764fa7545",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.piece_count:  110\n",
      "dataset_len:  11000\n",
      "self.piece_count:  110\n",
      "dataset_len:  11000\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "dataset_name_path = f\"./data_list_symbolic_cross_audio.txt\"\n",
    "dataloader = get_dataloader(dataset_name_path, batch_size=8) #[20, 512, 128], [20, 512, 102]\n",
    "dataset = AudioMotionDataSet(dataset_name_path)\n",
    "\n",
    "val_dataset_name_path = f\"./data_list_symbolic_cross_audio.txt\"\n",
    "# val_dataloader = get_val_dataloader(val_dataset_name_path, batch_size=40) #[20, 512, 128], [20, 512, 102]\n",
    "\n",
    "full_data_path = None\n",
    "with open(\"./data_list_symbolic_cross_audio.txt\", \"r\") as file:\n",
    "    lines = [line.strip() for line in file]\n",
    "    full_data_path = np.array(lines)\n",
    "\n",
    "val_data_read = np.reshape(full_data_path, (11, 10))\n",
    "# print(val_data_read)\n",
    "\n",
    "learning_rate = 0.001#0.001\n",
    "\n",
    "# input_size_encoder = 128 #129 #128\n",
    "# input_size_decoder = 115 #102 #24\n",
    "# output_size = 115#102 #24\n",
    "\n",
    "# encoder_embedding_size = 300\n",
    "# decoder_embedding_size = 300\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.\n",
    "step = 0\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2912b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(model): # reset the weight every fold\n",
    "    if isinstance(model, nn.LSTM) or isinstance(model, nn.Linear):\n",
    "        model.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cc5417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, output_dim, input_size, hidden_size, num_layers, seq_length):\n",
    "        super(LSTM1, self).__init__()\n",
    "        self.output_dim = output_dim #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        self.seq_length = seq_length #sequence length\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True) #lstm\n",
    "        self.fc_1 =  nn.Linear(hidden_size, output_dim) #fully connected to determine output dim\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        # h0, c0 no time information\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device) #internal state\n",
    "        # Propagate input through LSTM\n",
    "        # x is MIDI => [44, 512, 128]\n",
    "\n",
    "        # hn is final state, run over the sequence length\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        # hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        # print(\"output.shape\", output.shape)\n",
    "        # print(\"hn.shape\", hn.shape)\n",
    "        # out = self.relu(hn)\n",
    "        out = self.fc_1(output) #final\n",
    "        return out\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23df2697-2943-4f69-89df-1f0c5cbf3343",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "input_size = 156 #number of features\n",
    "hidden_size = 1024 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers\n",
    "seq_len = 512\n",
    "output_dim = 115 #number of output classes\n",
    "batch_size_define = 20#128\n",
    "\n",
    "# model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, tie_weights).to(device)\n",
    "# model = LSTM(embedding_dim, hidden_dim, num_layers, dropout_rate, tie_weights).to(device)\n",
    "# model = LSTM1(output_dim, input_size, hidden_size, num_layers, seq_len).to(device) #our lstm class\n",
    "# model.train()\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
    "\n",
    "num_epochs = 300#100 #10\n",
    "k_folds = 11\n",
    "cross_valid_results = {}\n",
    "torch.manual_seed(42)\n",
    "\n",
    "avg_loss_list = []\n",
    "all_loss_list = []\n",
    "val_loss_per_epoch_list = []\n",
    "\n",
    "#TODO: important cross val record\n",
    "val_time_loss_list = []\n",
    "val_dim_loss_list = []\n",
    "val_mse_loss_list = []\n",
    "val_per_split_list = [] #just mse loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56bd6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_wise_loss_fn(preds, labels):\n",
    "    '''\n",
    "    calculate time-wise loss for motion (along the time axis)\n",
    "    input: labels[batch, time, dimension(joint*xyz)]\n",
    "    preds[batch, time , dimension(joint*xyz)]\n",
    "    output: time loss\n",
    "    '''\n",
    "    # points_2 = ax.scatter(column(each_frame[30:32], 0), column(each_frame[30:32], 1), column(each_frame[30:32], 2), cmap='jet', marker='o', label='body joint', color = 'blue')\n",
    "    # points_3 = ax.scatter(column(each_frame[32:34], 0), column(each_frame[32:34], 1), column(each_frame[32:34], 2), cmap='jet', marker='o', label='body joint', color = 'red')\n",
    "    # [8, 9], [9, 11], [9, 10], [10, 11], [10, 12], [9, 12], [11, 12], #right hand\n",
    "    # [13, 14], [14, 16], [14, 15], [16, 15], [14, 17], [16, 17], [15, 17], #left hand\n",
    "    # [30, 31], [32, 33],  #instrument\n",
    "    \n",
    "    # print(\"preds.shape\", preds.shape)\n",
    "    # print(\"labels.shape\", labels.shape)\n",
    "    \n",
    "    # print(\"preds[9*3:18*3]\", preds[:, :, 9*3:19*3].shape)\n",
    "    # print(\"labels[9*3:18*3]\", labels[:, :, 9*3:19*3].shape)\n",
    "    \n",
    "    select_joint_preds = torch.cat((preds[:, :, 9*3:19*3], preds[:, :, 31*3:35*3]), 2)\n",
    "    select_joint_labels = torch.cat((labels[:, :, 9*3:19*3], labels[:, :, 31*3:35*3]), 2)\n",
    "    \n",
    "    # print(\"select_joint_preds.shape\", select_joint_preds.shape)\n",
    "    # # print(select_joint_preds)\n",
    "    # print(\"select_joint_labels.shape\", select_joint_labels.shape)\n",
    "    \n",
    "    epsilon = 1e-7\n",
    "    select_joint_preds = select_joint_preds + epsilon\n",
    "\n",
    "    labels_transpose = torch.permute(select_joint_labels, (0, 2, 1))#tf.transpose(labels, [0, 2, 1]) # [b, 3, t]\n",
    "    preds_transpose = torch.permute(select_joint_preds, (0, 2, 1))#tf.transpose(preds, [0, 2, 1]) # [b, 3, t]\n",
    "    # print(\"labels_transpose.shape\", labels_transpose.shape)\n",
    "    # print(\"preds_transpose.shape\", preds_transpose.shape)\n",
    "    # print(\"labels_transpose[:, :, :, None].shape\", labels_transpose[:, :, :, None].shape)\n",
    "    # print(\"labels_transpose[:, :, None, :].shape\", labels_transpose[:, :, None, :].shape)\n",
    "    label_diff = labels_transpose[:, :, :, None] - labels_transpose [:, :, None, :] # [b, 3, t, t]\n",
    "    \n",
    "    preds_diff = preds_transpose[:, :, :, None] - preds_transpose [:, :, None, :] # [b, 3, t, t]\n",
    "    # print(preds_diff.shape)\n",
    "    time_loss = (preds_diff - label_diff)**2 # [b, 3, t, t]\n",
    "    time_loss_value = time_loss.mean() #float()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return time_loss_value\n",
    "    \n",
    "def dim_wise_loss_fn(preds, labels):\n",
    "    '''\n",
    "    calculate dimension-wise loss for motion (along the dimension axis)\n",
    "    input: labels[batch, time, dimension(joint*xyz)]\n",
    "    preds[batch, time , dimension(joint*xyz)]\n",
    "    output: dimension loss\n",
    "    '''\n",
    "    select_joint_preds = torch.cat((preds[:, :, 9*3:19*3], preds[:, :, 31*3:35*3]), 2)\n",
    "    select_joint_labels = torch.cat((labels[:, :, 9*3:19*3], labels[:, :, 31*3:35*3]), 2)\n",
    "\n",
    "    epsilon = 1e-7\n",
    "    preds = preds + epsilon\n",
    "    \n",
    "    label_diff = select_joint_labels[:, :, :, None] - select_joint_labels[:, :, None, :] # [b, t, 3, 3]\n",
    "    preds_diff = select_joint_preds[:, :, :, None] - select_joint_preds[:, :, None, :] # [b, t, 3, 3]\n",
    "    dim_loss = (preds_diff - label_diff)**2 # [b, t, 3, 3]\n",
    "    dim_loss_value = dim_loss.mean() #float()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return dim_loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5df5bbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def time_wise_loss_fn(preds, labels):\n",
    "#     '''\n",
    "#     calculate time-wise loss for motion (along the time axis)\n",
    "#     input: labels[batch, time, dimension(joint*xyz)]\n",
    "#     preds[batch, time , dimension(joint*xyz)]\n",
    "#     output: time loss\n",
    "#     '''\n",
    "#     epsilon = 1e-7\n",
    "#     preds = preds + epsilon\n",
    "\n",
    "#     labels_transpose = torch.permute(labels, (0, 2, 1))#tf.transpose(labels, [0, 2, 1]) # [b, 3, t]\n",
    "#     preds_transpose = torch.permute(preds, (0, 2, 1))#tf.transpose(preds, [0, 2, 1]) # [b, 3, t]\n",
    "#     # print(\"labels_transpose.shape\", labels_transpose.shape)\n",
    "#     # print(\"preds_transpose.shape\", preds_transpose.shape)\n",
    "#     # print(\"labels_transpose[:, :, :, None].shape\", labels_transpose[:, :, :, None].shape)\n",
    "#     # print(\"labels_transpose[:, :, None, :].shape\", labels_transpose[:, :, None, :].shape)\n",
    "#     label_diff = labels_transpose[:, :, :, None] - labels_transpose [:, :, None, :] # [b, 3, t, t]\n",
    "    \n",
    "#     preds_diff = preds_transpose[:, :, :, None] - preds_transpose [:, :, None, :] # [b, 3, t, t]\n",
    "#     # print(preds_diff.shape)\n",
    "#     time_loss = (preds_diff - label_diff)**2 # [b, 3, t, t]\n",
    "#     time_loss_value = time_loss.mean() #float()\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "#     return time_loss_value\n",
    "    \n",
    "# def dim_wise_loss_fn(preds, labels):\n",
    "#     '''\n",
    "#     calculate dimension-wise loss for motion (along the dimension axis)\n",
    "#     input: labels[batch, time, dimension(joint*xyz)]\n",
    "#     preds[batch, time , dimension(joint*xyz)]\n",
    "#     output: dimension loss\n",
    "#     '''\n",
    "#     epsilon = 1e-7\n",
    "#     preds = preds + epsilon\n",
    "    \n",
    "#     label_diff = labels[:, :, :, None] - labels[:, :, None, :] # [b, t, 3, 3]\n",
    "#     preds_diff = preds[:, :, :, None] - preds[:, :, None, :] # [b, t, 3, 3]\n",
    "#     dim_loss = (preds_diff - label_diff)**2 # [b, t, 3, 3]\n",
    "#     dim_loss_value = dim_loss.mean() #float()\n",
    "#     torch.cuda.empty_cache()\n",
    "    \n",
    "#     return dim_loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52fb7938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def customized_mse_loss(output, target):\n",
    "    # target = target.transpose(0, 1)\n",
    "\n",
    "    # print(\"output.shape:\", output.shape) #torch.Size([20, 513, 102])\n",
    "    # print(\"target.shape:\", target.shape) #torch.Size([20, 513, 102])\n",
    "\n",
    "    w1_time = 0.3\n",
    "    w2_dim = 0.3\n",
    "    w3_mse = 0.4\n",
    "\n",
    "    mse_loss = F.mse_loss(output, target)\n",
    "    time_loss = time_wise_loss_fn(output, target)\n",
    "    dim_loss = dim_wise_loss_fn(output, target)\n",
    "\n",
    "    # print(\"time_loss:\", time_loss)\n",
    "    # print(\"dim_loss:\", dim_loss)\n",
    "    # print(\"mse_loss:\", mse_loss)\n",
    "    val_time_loss_list.append(time_loss.cpu().item())\n",
    "    val_dim_loss_list.append(dim_loss.cpu().item())\n",
    "    val_mse_loss_list.append(mse_loss.cpu().item())\n",
    "\n",
    "    segment_loss = (w1_time * time_loss) + (w2_dim * dim_loss) + (w3_mse * mse_loss)\n",
    "    torch.cuda.empty_cache()\n",
    "    return  segment_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee089d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lstm_cross(model, split_count):\n",
    "    model.eval()\n",
    "    print('Validation')\n",
    "    valid_running_loss = 0.0\n",
    "    counter = 0\n",
    "    # previous_output = torch.zeros(512, 102).to(device)\n",
    "    \n",
    "    outputs_save = []\n",
    "    \n",
    "    np.savetxt('./temp_path.txt', val_data_read[split_count], delimiter=\"\\n\", fmt=\"%s\")\n",
    "    # print(val_data_read[split_count])\n",
    "    val_dataloader = get_audio_val_dataloader('./temp_path.txt', batch_size=11)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, targets) in enumerate(val_dataloader): #tqdm(enumerate(val_dataloader), total=len(val_dataloader))\n",
    "            counter += 1\n",
    "\n",
    "            inputs = inputs.to(device).float()\n",
    "            targets = targets.to(device).float()\n",
    "            # print(\"val inputs.shape:\", inputs.shape)\n",
    "            # print(\"val targets.shape:\", targets.shape)\n",
    "            outputs = model(inputs)\n",
    "            # print(\"val outputs.shape:\", outputs.shape)\n",
    "\n",
    "            loss =  F.mse_loss(outputs, targets)\n",
    "            valid_running_loss += loss.cpu().item()\n",
    "            # previous_output = outputs\n",
    "            outputs_save.append(np.asarray(outputs.cpu()))\n",
    "\n",
    "    loc_dt = datetime.datetime.today()\n",
    "    loc_dt_format = loc_dt.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    if not os.path.exists(\"./output_eval/\"):\n",
    "        os.makedirs('./output_eval/')\n",
    "\n",
    "    # print(\"counter\", counter)\n",
    "    # print(\"val_data_read[split_count][counter]\", val_data_read[split_count][counter])\n",
    "    val_file_name = val_data_read[split_count][counter].split('/')[2].split('.')[0]\n",
    "\n",
    "    # print(\"val file_name:\", val_file_name)\n",
    "    # print(\"outputs_save length: \", len(outputs_save), \", element shape: \" , outputs_save[0].shape)\n",
    "    #                                            str(split_count)\n",
    "    eval_output = open(\"./output_eval/[split_\" + str(6) + \"][audio_with_anno][total\" + str(num_epochs) + \"_hs\" + str(hidden_size) +\"]save_\"+ str(loc_dt_format) + \"_l1_loss_\" + str(loss.cpu().item())+\".pkl\", 'wb')\n",
    "    pickle.dump(np.asarray(outputs_save), eval_output)\n",
    "    eval_output.close()\n",
    "    \n",
    "    # print(\"val counter:\", counter)\n",
    "    epoch_val_loss_f1 = valid_running_loss / counter\n",
    "    val_per_split_list.append(epoch_val_loss_f1)\n",
    "    print(\"split_count:\", split_count, \", epoch_val_loss:\", epoch_val_loss_f1)\n",
    "    os.remove(\"./temp_path.txt\")\n",
    "    return #epoch_val_loss_f1\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14342657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "random:  6\n",
      "------------fold no---------6----------------------\n",
      "train_idx: 0 ~ 10999  test_idx: 6000 ~ 6999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Starting epoch 2\n",
      "Starting epoch 3\n",
      "Starting epoch 4\n",
      "Starting epoch 5\n",
      "Starting epoch 6\n",
      "Starting epoch 7\n",
      "Starting epoch 8\n",
      "Starting epoch 9\n",
      "Starting epoch 10\n",
      "Starting epoch 11\n",
      "Starting epoch 12\n",
      "Starting epoch 13\n",
      "Starting epoch 14\n",
      "Starting epoch 15\n",
      "Starting epoch 16\n",
      "Starting epoch 17\n",
      "Starting epoch 18\n",
      "Starting epoch 19\n",
      "Starting epoch 20\n",
      "Starting epoch 21\n",
      "Starting epoch 22\n",
      "Starting epoch 23\n",
      "Starting epoch 24\n",
      "Starting epoch 25\n",
      "Starting epoch 26\n",
      "Starting epoch 27\n",
      "Starting epoch 28\n",
      "Starting epoch 29\n",
      "Starting epoch 30\n",
      "Starting epoch 31\n",
      "Starting epoch 32\n",
      "Starting epoch 33\n",
      "Starting epoch 34\n",
      "Starting epoch 35\n",
      "Starting epoch 36\n",
      "Starting epoch 37\n",
      "Starting epoch 38\n",
      "Starting epoch 39\n",
      "Starting epoch 40\n",
      "Starting epoch 41\n",
      "Starting epoch 42\n",
      "Starting epoch 43\n",
      "Starting epoch 44\n",
      "Starting epoch 45\n",
      "Starting epoch 46\n",
      "Starting epoch 47\n",
      "Starting epoch 48\n",
      "Starting epoch 49\n",
      "Starting epoch 50\n",
      "Starting epoch 51\n",
      "Starting epoch 52\n",
      "Starting epoch 53\n",
      "Starting epoch 54\n",
      "Starting epoch 55\n",
      "Starting epoch 56\n",
      "Starting epoch 57\n",
      "Starting epoch 58\n",
      "Starting epoch 59\n",
      "Starting epoch 60\n",
      "Starting epoch 61\n",
      "Starting epoch 62\n",
      "Starting epoch 63\n",
      "Starting epoch 64\n",
      "Starting epoch 65\n",
      "Starting epoch 66\n",
      "Starting epoch 67\n",
      "Starting epoch 68\n",
      "Starting epoch 69\n",
      "Starting epoch 70\n",
      "Starting epoch 71\n",
      "Starting epoch 72\n",
      "Starting epoch 73\n",
      "Starting epoch 74\n",
      "Starting epoch 75\n",
      "Starting epoch 76\n",
      "Starting epoch 77\n",
      "Starting epoch 78\n",
      "Starting epoch 79\n",
      "Starting epoch 80\n",
      "Starting epoch 81\n",
      "Starting epoch 82\n",
      "Starting epoch 83\n",
      "Starting epoch 84\n",
      "Starting epoch 85\n",
      "Starting epoch 86\n",
      "Starting epoch 87\n",
      "Starting epoch 88\n",
      "Starting epoch 89\n",
      "Starting epoch 90\n",
      "Starting epoch 91\n",
      "Starting epoch 92\n",
      "Starting epoch 93\n",
      "Starting epoch 94\n",
      "Starting epoch 95\n",
      "Starting epoch 96\n",
      "Starting epoch 97\n",
      "Starting epoch 98\n",
      "Starting epoch 99\n",
      "Starting epoch 100\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 1 FOLDS\n",
      "--------------------------------\n",
      "Fold loss 0: 0.01638113685697317\n",
      "Average vaildation new loss: 0.01638113685697317\n",
      "Validation\n",
      "val_dataset_len 10\n",
      "len(audio_data) 7380\n",
      "len(motion_data) 7380\n",
      "len(audio_data) 8276\n",
      "len(motion_data) 8276\n",
      "len(audio_data) 6255\n",
      "len(motion_data) 6255\n",
      "len(audio_data) 6153\n",
      "len(motion_data) 6153\n",
      "len(audio_data) 3548\n",
      "len(motion_data) 3548\n",
      "len(audio_data) 8925\n",
      "len(motion_data) 8925\n",
      "len(audio_data) 6599\n",
      "len(motion_data) 6599\n",
      "len(audio_data) 7428\n",
      "len(motion_data) 7428\n",
      "len(audio_data) 3850\n",
      "len(motion_data) 3850\n",
      "len(audio_data) 7145\n",
      "len(motion_data) 7145\n",
      "split_count: 0 , epoch_val_loss: 0.0909089595079422\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=k_folds)\n",
    "print(kf.get_n_splits(dataset))\n",
    "KFold(n_splits=k_folds, random_state=None, shuffle=False)\n",
    "# for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "#     #...TODO\n",
    "split_count = 0\n",
    "random_pick_fold = 6#random.randint(0, 10) #0~10\n",
    "print(\"random: \", random_pick_fold)\n",
    "# for fold,(train_idx, test_idx) in enumerate(kf.split(dataset)): #TODO: random pick 1 fold\n",
    "for (train_idx, test_idx) in itertools.islice(kf.split(dataset), random_pick_fold, random_pick_fold+1):\n",
    "    # print('------------fold no---------{}----------------------'.format(fold))\n",
    "    print('------------fold no---------{}----------------------'.format(random_pick_fold))\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "    print(\"train_idx:\", train_idx[0], \"~\", train_idx[-1], \" test_idx:\", test_idx[0], \"~\", test_idx[-1])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "                        dataset,\n",
    "                        num_workers=0,\n",
    "                        pin_memory=False,\n",
    "                        drop_last=False,\n",
    "                        batch_size=batch_size_define, sampler=train_subsampler) #bs=40:4.49G, bs=128:14.65G\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "                        dataset,\n",
    "                        num_workers=0,\n",
    "                        pin_memory=False,\n",
    "                        drop_last=False,\n",
    "                        batch_size=batch_size_define, sampler=val_subsampler)\n",
    "    \n",
    "    model = LSTM1(output_dim, input_size, hidden_size, num_layers, seq_len).to(device) #our lstm class\n",
    "    model.apply(reset_weights)\n",
    "    \n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "        losses = []\n",
    "        loss = 0\n",
    "        mean_loss = 0\n",
    "        for i, (audio_batch, motion_batch) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            \n",
    "            audio_batch = audio_batch.to(device).float()\n",
    "            motion_batch = motion_batch.to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(audio_batch) #midi_batch\n",
    "            # print(\"train inputs.shape:\", midi_batch.shape, torch.isnan(midi_batch).any())\n",
    "            # # print(motion_batch)\n",
    "            # print(\"train targets.shape:\", motion_batch.shape, torch.isnan(motion_batch).any())\n",
    "            # print(\"train outputs.shape:\", output.shape, torch.isnan(output).any())\n",
    "\n",
    "            # loss =  F.mse_loss(output, motion_batch)\n",
    "            # loss = customized_mse_loss(output.cpu(), motion_batch.cpu())\n",
    "            loss = customized_mse_loss(output, motion_batch)\n",
    "            \n",
    "            losses.append(loss.cpu().item()) #.cpu().item()\n",
    "            all_loss_list.append(loss.cpu().item()) #.cpu().item()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # print(f\"Epoch {epoch}, batch {i}: loss = {loss.cpu().item():.6f}\") #.cpu().item()\n",
    "\n",
    "        # print(losses, sum(losses), len(losses))\n",
    "        mean_loss = sum(losses)/len(losses)\n",
    "        # correct, total = 0, 0\n",
    "        valid_running_loss = 0.0\n",
    "        counter = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (audio_test, motion_test) in enumerate(val_loader):\n",
    "                \n",
    "                inputs = audio_test.to(device).float()\n",
    "                targets = motion_test.to(device).float()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                # print(\"val inputs.shape:\", inputs.shape)\n",
    "                # print(\"val targets.shape:\", targets.shape)\n",
    "                # print(\"val outputs.shape:\", outputs.shape)\n",
    "\n",
    "                val_loss =  customized_mse_loss(outputs, targets)\n",
    "                valid_running_loss += val_loss.cpu().item() #.cpu().item()\n",
    "                counter += 1\n",
    "            \n",
    "            epoch_val_loss = valid_running_loss / counter\n",
    "            # print(f\"Epoch {epoch}: val_loss = {epoch_val_loss:.6f}\") #.cpu().item()\n",
    "\n",
    "        avg_loss_list.append(mean_loss) #.cpu().item()\n",
    "        val_loss_per_epoch_list.append(epoch_val_loss) #.cpu().item()\n",
    "\n",
    "        cross_valid_results[0] = epoch_val_loss\n",
    "        \n",
    "        loc_dt = datetime.datetime.today()\n",
    "        loc_dt_format = loc_dt.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        if (epoch+1)%100 == 0:\n",
    "            torch.save({\n",
    "                'epoch':epoch,\n",
    "                'model_state_dict':model.state_dict(),\n",
    "                'optimizer_state_dict':optimizer.state_dict(),\n",
    "                'loss':loss\n",
    "            },  \"./model_save/[audio_with_anno][total\"+str(num_epochs)+ \"_hs\" + str(hidden_size) +\"]LSTM_save_epoch_\" + str(epoch)+ \"_\"+ str(loc_dt_format) + \"_avg_loss_\" + str(mean_loss) +\".tar\")\n",
    "\n",
    "    # Print fold results\n",
    "    print(f'K-FOLD CROSS VALIDATION RESULTS FOR {1} FOLDS')\n",
    "    print('--------------------------------')\n",
    "    sum_loss = 0.0\n",
    "    for key, value in cross_valid_results.items():\n",
    "        print(f'Fold loss {key}: {value}')\n",
    "        sum_loss += value\n",
    "    print(f'Average vaildation new loss: {sum_loss/len(cross_valid_results.items())}')\n",
    "    \n",
    "    # validation result save\n",
    "    evaluate_lstm_cross(model, split_count)\n",
    "    \n",
    "    split_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5f00152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preprocessed_data_save_cross_aud/audio/vio01_Elgar_S1_T1_audio_data.pkl'\n",
      " 'preprocessed_data_save_cross_aud/audio/vio01_Elgar_S1_T2_audio_data.pkl'\n",
      " 'preprocessed_data_save_cross_aud/audio/vio01_Flower_S1_T1_audio_data.pkl'\n",
      " 'preprocessed_data_save_cross_aud/audio/vio01_Flower_S1_T2_audio_data.pkl'\n",
      " 'preprocessed_data_save_cross_aud/audio/vio01_Mend_S1_T1_audio_data.pkl'\n",
      " 'preprocessed_data_save_cross_aud/audio/vio01_Mend_S1_T2_audio_data.pkl'\n",
      " 'preprocessed_data_save_cross_aud/audio/vio01_Mozart1_S1_T1_audio_data.pkl'\n",
      " 'preprocessed_data_save_cross_aud/audio/vio01_Mozart1_S1_T2_audio_data.pkl'\n",
      " 'preprocessed_data_save_cross_aud/audio/vio01_Mozart2_S1_T1_audio_data.pkl'\n",
      " 'preprocessed_data_save_cross_aud/audio/vio01_Mozart2_S1_T2_audio_data.pkl']\n"
     ]
    }
   ],
   "source": [
    "print(val_data_read[split_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42ed6c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-18_00-02-01\n",
      "[0.022574301759898663, 0.021180258623510598, 0.020839990484714507, 0.020409348321706055, 0.020016615065932275, 0.0198870617069304, 0.019450549481064082, 0.019248797840625047, 0.019049027463048696, 0.018804145911335945, 0.018564312371611595, 0.01836824054569006, 0.018364610700309276, 0.018073319456726314, 0.01807970843911171, 0.01788466232419014, 0.01761753334030509, 0.01764287444502115, 0.01759130107164383, 0.017453278943151237, 0.01741305451244116, 0.017504273404926062, 0.01743151963725686, 0.017119019469618798, 0.01713726275190711, 0.017259640707820652, 0.01719920119345188, 0.01699799572825432, 0.017544726924598218, 0.017221683264523745, 0.01744527061879635, 0.01733361777961254, 0.01716467973664403, 0.016750868076086046, 0.016833669143915177, 0.016889546924829482, 0.016740041889995335, 0.016712463850528003, 0.01717237947806716, 0.016601429896056653, 0.016565701138973238, 0.016818792567402124, 0.01699169590473175, 0.016882372639328243, 0.0164340042129159, 0.01675846779346466, 0.016411463229358197, 0.01634526923596859, 0.016180326899141072, 0.016330517786741258, 0.01620328754633665, 0.016066782215237618, 0.016260220953077078, 0.016181944423168896, 0.01608100193440914, 0.015880290895700453, 0.016059668482840062, 0.016760129008442162, 0.01689873579069972, 0.016514622112363576, 0.016537115371972323, 0.016321507473289965, 0.016014943435788156, 0.016034133492410185, 0.016899365485459564, 0.016442109083384275, 0.016255552729964255, 0.01618628908470273, 0.016181615092605354, 0.01670889154598117, 0.01761945104897022, 0.017641259829699992, 0.017625945602357387, 0.0175085744895041, 0.01705792677477002, 0.017770836149156092, 0.017196843943744898, 0.0170671298854053, 0.01710685139596462, 0.01867372754365206, 0.018090323736518622, 0.018004287149012088, 0.017650222355872394, 0.01735693828612566, 0.017119341422617434, 0.017093286813050508, 0.01700653376057744, 0.017086948961019515, 0.01685054942443967, 0.01670913476347923, 0.0165696432210505, 0.01651785535812378, 0.016592624340951444, 0.01655100633725524, 0.01661223295778036, 0.016328124143183232, 0.016283820593357087, 0.016791165678203105, 0.016179942727088927, 0.01651694549322128]\n"
     ]
    }
   ],
   "source": [
    "print(loc_dt_format)\n",
    "print(avg_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "430c6845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.021298756062984467, 0.020573857106268405, 0.02026856142282486, 0.020617131255567075, 0.020115891113877297, 0.01919410962611437, 0.01929555121809244, 0.019033971309661866, 0.019028468400239945, 0.01865253648161888, 0.018609327659010886, 0.01811175514012575, 0.019116331584751605, 0.017741454474627973, 0.01824856188893318, 0.01761901331692934, 0.017547306597232817, 0.017348801307380198, 0.017990633338689804, 0.01707701237499714, 0.018065265774726866, 0.017024904541671278, 0.017062797397375106, 0.017436405465006827, 0.016586425423622132, 0.016738970212638378, 0.016700909212231635, 0.01708371753245592, 0.017221762388944625, 0.016514594592154028, 0.017774952247738837, 0.017545716650784016, 0.017161910980939867, 0.016487753115594386, 0.016614845924079417, 0.017591939598321914, 0.01684808924794197, 0.016310379542410373, 0.016558199487626553, 0.01675050626695156, 0.016746047526597977, 0.017143569014966487, 0.017348543509840966, 0.016665008388459683, 0.015998411938548087, 0.016436675876379012, 0.01653879312425852, 0.0162838623970747, 0.016410978376865386, 0.01642155184596777, 0.01559753503650427, 0.01644435413181782, 0.016115468136966227, 0.01639030174911022, 0.015742865808308124, 0.015864302679896355, 0.01574293303489685, 0.016492847785353662, 0.016483900859951973, 0.01615376890450716, 0.01575166391581297, 0.016529362104833127, 0.015950247414410115, 0.016186116732656956, 0.016552504979074, 0.0165990993604064, 0.0164236598610878, 0.016599183216691016, 0.01684187093377113, 0.01636553580313921, 0.0180177736133337, 0.017198531694710254, 0.017181955881416796, 0.01671969623863697, 0.01941229684650898, 0.01711000678688288, 0.017177428685128688, 0.017539286315441133, 0.016392880953848363, 0.018576901085674762, 0.017497803151607514, 0.01746748073399067, 0.017728050597012043, 0.01700569351762533, 0.016849290624260903, 0.0165469551384449, 0.017020593225955963, 0.016404742904007435, 0.01670092426240444, 0.0175481715798378, 0.016357255227863788, 0.016763327725231647, 0.016032458923757077, 0.016680463768541813, 0.016820612497627736, 0.016516697607934474, 0.018469079837203025, 0.015927517235279083, 0.0160398091673851, 0.01638113685697317]\n"
     ]
    }
   ],
   "source": [
    "print(val_loss_per_epoch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e5ec087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137500\n",
      "137500\n",
      "137500\n"
     ]
    }
   ],
   "source": [
    "# val_time_loss_list\n",
    "# val_dim_loss_list\n",
    "# val_mse_loss_list\n",
    "plt.cla()\n",
    "plt.clf()\n",
    "print(len(val_time_loss_list))\n",
    "val_time_loss_list_dataframe = pd.DataFrame(val_time_loss_list)\n",
    "plt.plot(np.array(val_time_loss_list_dataframe.index), np.array(val_time_loss_list_dataframe[0]))\n",
    "plt.savefig(\"avg_time_loss_training.jpg\")\n",
    "plt.show()\n",
    "\n",
    "plt.cla()\n",
    "plt.clf()\n",
    "print(len(val_dim_loss_list))\n",
    "val_dim_loss_list_dataframe = pd.DataFrame(val_dim_loss_list)\n",
    "plt.plot(np.array(val_dim_loss_list_dataframe.index), np.array(val_dim_loss_list_dataframe[0]))\n",
    "plt.savefig(\"avg_dim_loss_training.jpg\")\n",
    "plt.show()\n",
    "\n",
    "plt.cla()\n",
    "plt.clf()\n",
    "print(len(val_mse_loss_list))\n",
    "val_mse_loss_list_dataframe = pd.DataFrame(val_mse_loss_list)\n",
    "plt.plot(np.array(val_mse_loss_list_dataframe.index), np.array(val_mse_loss_list_dataframe[0]))\n",
    "plt.savefig(\"avg_mse_loss_training.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaa7c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84a25a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cla()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da5982f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(avg_loss_list))\n",
    "avg_loss_list_dataframe = pd.DataFrame(avg_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "363e0919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.022574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.020840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.020409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.020017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.016328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.016284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.016791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.016180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.016517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "0   0.022574\n",
       "1   0.021180\n",
       "2   0.020840\n",
       "3   0.020409\n",
       "4   0.020017\n",
       "..       ...\n",
       "95  0.016328\n",
       "96  0.016284\n",
       "97  0.016791\n",
       "98  0.016180\n",
       "99  0.016517\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_loss_list_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "268330eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(avg_loss_list_dataframe.index), np.array(avg_loss_list_dataframe[0]))\n",
    "plt.savefig(\"avg_loss_training.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51793ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cla()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62b02538",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list_dataframe = pd.DataFrame(all_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f51b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(loss_list_dataframe.index), np.array(loss_list_dataframe[0]))\n",
    "plt.savefig(\"training_loss.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bac154f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cla()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e5b749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_per_epoch_list_dataframe = pd.DataFrame(val_loss_per_epoch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ade5deaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(val_loss_per_epoch_list_dataframe.index), np.array(val_loss_per_epoch_list_dataframe[0]))\n",
    "plt.savefig(\"training_val_loss.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bd4e519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input = torch.as_tensor(input).to(torch.float32).to(device)\n",
    "        # print(target.shape)\n",
    "        # target = torch.as_tensor(target).to(torch.float32).to(device)\n",
    "        # TODO: target should be <sos>, should not random\n",
    "        outputs = model(input)\n",
    "        return outputs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12f49577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_midi(filename, specific_fps):\n",
    "    # Load the MIDI file\n",
    "    midi_data = pretty_midi.PrettyMIDI(filename)\n",
    "\n",
    "    piano_roll = midi_data.get_piano_roll(fs=specific_fps)  # 40fps #250fps\n",
    "    piano_roll[piano_roll > 0] = 1\n",
    "\n",
    "    return piano_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86e0a3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_preprocess(audio_path, specific_fps):\n",
    "    n_fft = 4096\n",
    "    hop = int(44000/specific_fps)  # 1102.5 -> 40fps #882 -> 50fps\n",
    "    y, sr = librosa.load(audio_path, sr=44000)  # 44000 for divide 40\n",
    "    print(\"y.shape\", y.shape)\n",
    "    print(\"sample rate: \", sr)\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y=y, sr=sr, n_fft=n_fft, hop_length=hop, n_mfcc=13)\n",
    "    y = np.where(y == 0, 1e-10, y)\n",
    "    energy = np.log(librosa.feature.rms(\n",
    "        y=y, frame_length=n_fft, hop_length=hop, center=True))\n",
    "    mfcc_energy = np.vstack((mfcc, energy))\n",
    "    mfcc_delta = librosa.feature.delta(mfcc_energy)\n",
    "\n",
    "    sgram = librosa.stft(y, n_fft=n_fft, hop_length=hop)\n",
    "    sgram_mag, _ = librosa.magphase(sgram)\n",
    "    mel_scale_sgram = librosa.feature.melspectrogram(\n",
    "        S=sgram_mag, sr=sr)\n",
    "\n",
    "    print(\"mfcc_energy\", mfcc_energy.shape)\n",
    "    print(\"mfcc_delta\", mfcc_delta.shape)\n",
    "    print(\"mel_scale_sgram\", mel_scale_sgram.shape)\n",
    "\n",
    "    aud = np.vstack((mfcc_energy, mfcc_delta, mel_scale_sgram)).T\n",
    "\n",
    "    print(\"hop:\", hop)\n",
    "    print(\"aud:\", aud.shape)\n",
    "    return aud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "270a77a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "str_name: ./BWV1001/vs1-1ada.wav\n",
      "filecode:  vs1-1ada\n",
      "./BWV1001/vs1-1ada.wav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.shape (8988979,)\n",
      "sample rate:  44000\n",
      "mfcc_energy (14, 8172)\n",
      "mfcc_delta (14, 8172)\n",
      "mel_scale_sgram (128, 8172)\n",
      "hop: 1100\n",
      "aud: (8172, 156)\n",
      "(8172, 156)\n",
      "str_name: ./BWV1001/vs1-3sic.wav\n",
      "filecode:  vs1-3sic\n",
      "./BWV1001/vs1-3sic.wav\n",
      "y.shape (7687297,)\n",
      "sample rate:  44000\n",
      "mfcc_energy (14, 6989)\n",
      "mfcc_delta (14, 6989)\n",
      "mel_scale_sgram (128, 6989)\n",
      "hop: 1100\n",
      "aud: (6989, 156)\n",
      "(6989, 156)\n",
      "str_name: ./BWV1001/vs1-2fug.wav\n",
      "filecode:  vs1-2fug\n",
      "./BWV1001/vs1-2fug.wav\n",
      "y.shape (12687134,)\n",
      "sample rate:  44000\n",
      "mfcc_energy (14, 11534)\n",
      "mfcc_delta (14, 11534)\n",
      "mel_scale_sgram (128, 11534)\n",
      "hop: 1100\n",
      "aud: (11534, 156)\n",
      "(11534, 156)\n",
      "str_name: ./BWV1001/vs1-4prs.wav\n",
      "filecode:  vs1-4prs\n",
      "./BWV1001/vs1-4prs.wav\n",
      "y.shape (8686945,)\n",
      "sample rate:  44000\n",
      "mfcc_energy (14, 7898)\n",
      "mfcc_delta (14, 7898)\n",
      "mel_scale_sgram (128, 7898)\n",
      "hop: 1100\n",
      "aud: (7898, 156)\n",
      "(7898, 156)\n"
     ]
    }
   ],
   "source": [
    "test_datapath = \"./BWV1001/\"\n",
    "change_fps = 40\n",
    "test_audio_path_list = glob.glob(test_datapath + \"*.wav\")\n",
    "test_data_list = []\n",
    "test_music_list = []\n",
    "for test_audio in test_audio_path_list:\n",
    "    str_name = test_audio\n",
    "    print(\"str_name:\", str_name)\n",
    "    filename = str_name.split('/')[2]\n",
    "    filecode = filename.split('.')[0]\n",
    "    print(\"filecode: \",filecode)\n",
    "    test_music_list.append(filecode)\n",
    "    \n",
    "    print(test_audio)\n",
    "    # read_piano_roll = read_midi(test_midi, change_fps)\n",
    "    read_audio = audio_preprocess(test_audio, change_fps)\n",
    "    # read_audio_transpose = read_audio\n",
    "    print(read_audio.shape)\n",
    "    test_audio_len = read_audio.shape[0]\n",
    "    test_data_list.append(read_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c66d8cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column(matrix, i):\n",
    "    return [row[i] for row in matrix]\n",
    "\n",
    "def test_render_animation(fps, output, azim, prediction, ground_truth=None):\n",
    "    prediction_array = np.asarray(prediction)\n",
    "    print(prediction_array.size)\n",
    "    limit = len(prediction_array)\n",
    "    print(\"limit\", limit)\n",
    "    size = 6#6\n",
    "    fps = 40\n",
    "\n",
    "    # Skeleton layout\n",
    "    parents = [[0, 1], [1, 3], [3, 2], [0, 2],#head\n",
    "                [8, 6], [6, 13], [13, 4], [4, 8],#shoulder\n",
    "                [6, 4], [4, 5], [5, 7], [7, 6],#Upper torso\n",
    "                [8, 18], [8, 20], [13, 21], [13, 19],\n",
    "                [5, 20], [5, 21], [7, 18], [7, 19],\n",
    "                [18, 19], [19, 21], [21, 20], [20, 18], #waist\n",
    "                [18, 22], [20, 22], [22, 23], [22, 25], [23, 25], [24,23], [24, 25],  #right lag\n",
    "                [21, 26], [19, 26], [26, 27], [26, 29], [27, 29], [28, 27], [28, 29], #left lag\n",
    "                [8, 9], [9, 11], [9, 10], [10, 11], [10, 12], [9, 12], [11, 12], #right hand\n",
    "                [13, 14], [14, 16], [14, 15], [16, 15], [14, 17], [16, 17], [15, 17], #left hand\n",
    "                [31, 33], [30, 32], [30, 31], [32, 33], [31, 32], [30, 33] #instrument\n",
    "                        ]\n",
    "    # joints_right = [1, 2, 12, 13, 14]\n",
    "\n",
    "    prediction_array[:, :, 2] += 0.1 #[:, :, 2]\n",
    "    if ground_truth is not None:\n",
    "        ground_truth[:, :, 2] += 0.1\n",
    "        poses = {'Prediction': prediction_array,\n",
    "                 'Ground_truth': ground_truth}\n",
    "    else:\n",
    "        poses = {'Prediction': prediction_array}\n",
    "    \n",
    "\n",
    "    fig = plt.figure()#(figsize=(size*len(poses), size))\n",
    "    # ax_3d = []\n",
    "    # lines_3d = []\n",
    "    radius = 1#14 #3.7#\n",
    "    # print(poses)\n",
    "    for index, (title, data) in enumerate(poses.items()):\n",
    "        ax = fig.add_subplot(1, len(poses), index + 1, projection='3d')\n",
    "        ax.clear()\n",
    "        print(data)\n",
    "        ims = [] #每一 frame 都存\n",
    "        for frame_index, each_frame in enumerate(data):\n",
    "            # print(\"each_frame\")\n",
    "            # print(each_frame)\n",
    "            ax.view_init(elev=15., azim=azim)\n",
    "            ax.set_xlim3d([-radius/2, radius/2])\n",
    "            ax.set_zlim3d([0, radius])\n",
    "            ax.set_ylim3d([-radius/2, radius/2])\n",
    "            ax.set_aspect('auto') #ax.set_aspect('equal')\n",
    "\n",
    "            # print(title)\n",
    "            points = ax.scatter(column(each_frame[:30], 0), column(each_frame[:30], 1), column(each_frame[:30], 2), cmap='jet', marker='o', label='body joint', color = 'black')\n",
    "            points_2 = ax.scatter(column(each_frame[30:32], 0), column(each_frame[30:32], 1), column(each_frame[30:32], 2), cmap='jet', marker='o', label='body joint', color = 'blue')\n",
    "            points_3 = ax.scatter(column(each_frame[32:34], 0), column(each_frame[32:34], 1), column(each_frame[32:34], 2), cmap='jet', marker='o', label='body joint', color = 'red')\n",
    "            \n",
    "            # ax.scatter(column(each_frame, 0), column(each_frame, 1), column(each_frame, 2), cmap='jet', marker='o', label='body joint')\n",
    "            # ax.legend()\n",
    "            # print(\"+++\")\n",
    "            \n",
    "            parents = [[0, 1], [1, 3], [3, 2], [0, 2],#head\n",
    "                        [8, 6], [6, 13], [13, 4], [4, 8],#shoulder\n",
    "                        [6, 4], [4, 5], [5, 7], [7, 6],#Upper torso\n",
    "                        [8, 18], [8, 20], [13, 21], [13, 19],\n",
    "                        [5, 20], [5, 21], [7, 18], [7, 19],\n",
    "                        [18, 19], [19, 21], [21, 20], [20, 18], #waist\n",
    "                        [18, 22], [20, 22], [22, 23], [22, 25], [23, 25], [24,23], [24, 25],  #right lag\n",
    "                        [21, 26], [19, 26], [26, 27], [26, 29], [27, 29], [28, 27], [28, 29], #left lag\n",
    "                        [8, 9], [9, 11], [9, 10], [10, 11], [10, 12], [9, 12], [11, 12], #right hand\n",
    "                        [13, 14], [14, 16], [14, 15], [16, 15], [14, 17], [16, 17], [15, 17], #left hand\n",
    "                        [30, 31], [32, 33],  #instrument\n",
    "                        # [31, 33], [30, 32], [30, 31], [32, 33], [31, 32], [30, 33] #instrument\n",
    "                        ]\n",
    "            lines = []\n",
    "            # draw line\n",
    "            \n",
    "            # lines = [ax.plot([each_frame[vs][0], each_frame[ve][0]],\n",
    "            #                  [each_frame[vs][1], each_frame[ve][1]],\n",
    "            #                  [each_frame[vs][2], each_frame[ve][2]]) for (vs, ve) in parents]\n",
    "            line_num = len(parents)\n",
    "            for idx, each_line in enumerate(parents):\n",
    "                vec_start = each_frame[each_line[0]]\n",
    "                vec_end = each_frame[each_line[1]]\n",
    "                # print(vec_start)\n",
    "                # print(vec_end)\n",
    "                line_color = \"black\"\n",
    "                if idx == line_num-2:\n",
    "                    line_color = \"blue\"\n",
    "                if idx == line_num-1:\n",
    "                    line_color = \"red\"\n",
    "                # ax.plot([vec_start[0], vec_end[0]], [vec_start[1], vec_end[1]], [vec_start[2], vec_end[2]])\n",
    "                \n",
    "                temp, = ax.plot([vec_start[0], vec_end[0]], [vec_start[1], vec_end[1]], [vec_start[2], vec_end[2]], color=line_color)\n",
    "                lines.append(temp)\n",
    "\n",
    "            # ax.figure.savefig('./test_pic/pic' + str(frame_index) + '.png', dpi=100, bbox_inches = 'tight')\n",
    "\n",
    "            # ims.append([points])\n",
    "            # image_frame = [points].extend(lines)\n",
    "            ims.append([points]+[points_2]+[points_3]+lines) #TODO: try extend\n",
    "\n",
    "            # plt.cla()\n",
    "            # print(\"+++\")\n",
    "\n",
    "    anim = matplotlib.animation.ArtistAnimation(fig, ims, interval=1000/fps)\n",
    "\n",
    "    if output.endswith('.mp4'):\n",
    "        FFwriter = matplotlib.animation.FFMpegWriter(fps=fps, extra_args=['-vcodec', 'libx264'])\n",
    "        anim.save(output, writer=FFwriter)\n",
    "    elif output.endswith('.gif'):\n",
    "        anim.save(output, fps=fps, dpi=100, writer='imagemagick')\n",
    "    else:\n",
    "        raise ValueError('Unsupported output format (only .mp4 and .gif are supported)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "791b9843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(audio_path, plot_path, prediction, sample_time, fps, name=\"\"): #audio_path, plot_path, \n",
    "    # render_animation(fps, output='new_temp.mp4', azim=75, prediction=prediction)\n",
    "    test_render_animation(fps, output='new_temp_' + name + '.mp4', azim=75, prediction=prediction)\n",
    "\n",
    "    # # #merge with wav\n",
    "    input_video = ffmpeg.input('new_temp_' + name + '.mp4')\n",
    "    fluid_syn = FluidSynth()\n",
    "    fluid_syn.midi_to_audio(audio_path, './output' + name + '.wav')\n",
    "    input_audio = ffmpeg.input('./output' + name + '.wav')\n",
    "    # output = ffmpeg.output(video, audio, plot_path, vcodec='copy', acodec='aac', strict='experimental')\n",
    "    ffmpeg.concat(input_video, input_audio, v=1, a=1).output(plot_path).run()\n",
    "    # os.remove('new_temp_' + name + '.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca70a144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_input (1, 8172, 156)\n",
      "prediction.shape (1, 8172, 102)\n",
      "full_prediction (8172, 102)\n",
      "81600\n",
      "limit 800\n",
      "[[[ 0.07024748  0.13829876  1.08953187]\n",
      "  [ 0.00320912  0.10406311  1.10779772]\n",
      "  [ 0.10916515  0.06320114  1.08616326]\n",
      "  ...\n",
      "  [ 0.00599956  0.09332527  0.95309654]\n",
      "  [-0.15569627  0.05236987  1.08529637]\n",
      "  [ 0.08991085  0.2768048   0.79241619]]\n",
      "\n",
      " [[ 0.07078319  0.13274395  1.08814154]\n",
      "  [ 0.01116113  0.09102634  1.10696743]\n",
      "  [ 0.11871173  0.0642844   1.08729217]\n",
      "  ...\n",
      "  [ 0.01844802  0.08720319  0.95615945]\n",
      "  [-0.13366577  0.03363332  1.06781969]\n",
      "  [ 0.1143583   0.25749567  0.77368329]]\n",
      "\n",
      " [[ 0.06853681  0.14032692  1.09137473]\n",
      "  [ 0.00629252  0.09998175  1.11210392]\n",
      "  [ 0.11808024  0.06919579  1.09514359]\n",
      "  ...\n",
      "  [ 0.01587716  0.09266126  0.95754907]\n",
      "  [-0.14618626  0.01775675  1.05718282]\n",
      "  [ 0.1175925   0.27764222  0.80062178]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.02798912  0.14885654  1.0945401 ]\n",
      "  [-0.03520807  0.11679058  1.10939167]\n",
      "  [ 0.07607566  0.08289334  1.10835061]\n",
      "  ...\n",
      "  [-0.01866166  0.10036014  0.96362475]\n",
      "  [-0.14544261  0.11159235  1.10024426]\n",
      "  [ 0.07787089  0.20889586  0.7348629 ]]\n",
      "\n",
      " [[ 0.02555877  0.16163775  1.09425167]\n",
      "  [-0.03811533  0.13149147  1.11091671]\n",
      "  [ 0.0728077   0.09541845  1.11051354]\n",
      "  ...\n",
      "  [-0.02439705  0.10600105  0.96951661]\n",
      "  [-0.15153998  0.12236407  1.10681305]\n",
      "  [ 0.0674473   0.20970975  0.73129592]]\n",
      "\n",
      " [[ 0.02428832  0.17152202  1.09401134]\n",
      "  [-0.03991354  0.14304048  1.11212239]\n",
      "  [ 0.07069419  0.10511315  1.11238143]\n",
      "  ...\n",
      "  [-0.02804492  0.11018597  0.974318  ]\n",
      "  [-0.15692174  0.12801947  1.10894451]\n",
      "  [ 0.06103302  0.21408734  0.73075852]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fluidsynth: panic: An error occurred while reading from stdin.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FluidSynth runtime version 2.1.1\n",
      "Copyright (C) 2000-2020 Peter Hanappe and others.\n",
      "Distributed under the LGPL license.\n",
      "SoundFont(R) is a registered trademark of E-mu Systems, Inc.\n",
      "\n",
      "Rendering audio to file './outputvs1-1ada.wav'..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.2.2 Copyright (c) 2000-2019 the FFmpeg developers\n",
      "  built with gcc 7.3.0 (crosstool-NG 1.23.0.449-a04d0)\n",
      "  configuration: --prefix=/home/ilc/anaconda3/envs/sinica --cc=/tmp/build/80754af9/ffmpeg_1587154242452/_build_env/bin/x86_64-conda_cos6-linux-gnu-cc --disable-doc --enable-avresample --enable-gmp --enable-hardcoded-tables --enable-libfreetype --enable-libvpx --enable-pthreads --enable-libopus --enable-postproc --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame --disable-nonfree --enable-gpl --enable-gnutls --disable-openssl --enable-libopenh264 --enable-libx264\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'new_temp_vs1-1ada.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf58.29.100\n",
      "  Duration: 00:00:20.00, start: 0.000000, bitrate: 267 kb/s\n",
      "    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 640x480, 263 kb/s, 40 fps, 40 tbr, 10240 tbn, 80 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "Guessed Channel Layout for Input Stream #1.0 : stereo\n",
      "Input #1, wav, from './outputvs1-1ada.wav':\n",
      "  Duration: 00:03:24.29, bitrate: 1411 kb/s\n",
      "    Stream #1:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, stereo, s16, 1411 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 (h264) -> concat:in0:v0\n",
      "  Stream #1:0 (pcm_s16le) -> concat:in0:a0\n",
      "  concat:out:a0 -> Stream #0:0 (aac)\n",
      "  concat:out:v0 -> Stream #0:1 (libx264)\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x55b05e36d940] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "[libx264 @ 0x55b05e36d940] profile High, level 3.1, 4:2:0, 8-bit\n",
      "[libx264 @ 0x55b05e36d940] 264 - core 157 - H.264/MPEG-4 AVC codec - Copyleft 2003-2018 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=15 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to './video_vs1-1ada_test_predict.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf58.29.100\n",
      "    Stream #0:0: Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s (default)\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 aac\n",
      "    Stream #0:1: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 640x480, q=-1--1, 40 fps, 10240 tbn, 40 tbc (default)\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
      "frame=  800 fps=389 q=-1.0 Lsize=    3855kB time=00:03:24.31 bitrate= 154.6kbits/s speed=99.2x    \n",
      "video:601kB audio:3202kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 1.370074%\n",
      "[aac @ 0x55b05e36c200] Qavg: 182.785\n",
      "[libx264 @ 0x55b05e36d940] frame I:4     Avg QP:16.10  size: 13695\n",
      "[libx264 @ 0x55b05e36d940] frame P:231   Avg QP:23.77  size:  1348\n",
      "[libx264 @ 0x55b05e36d940] frame B:565   Avg QP:29.18  size:   439\n",
      "[libx264 @ 0x55b05e36d940] consecutive B-frames:  2.9%  6.5%  7.1% 83.5%\n",
      "[libx264 @ 0x55b05e36d940] mb I  I16..4: 28.1% 47.1% 24.7%\n",
      "[libx264 @ 0x55b05e36d940] mb P  I16..4:  0.0%  0.0%  0.0%  P16..4:  1.1%  1.9%  2.2%  0.0%  0.0%    skip:94.7%\n",
      "[libx264 @ 0x55b05e36d940] mb B  I16..4:  0.0%  0.0%  0.0%  B16..8:  1.7%  1.8%  1.0%  direct: 0.3%  skip:95.1%  L0:46.4% L1:41.4% BI:12.2%\n",
      "[libx264 @ 0x55b05e36d940] 8x8 transform intra:47.1% inter:23.8%\n",
      "[libx264 @ 0x55b05e36d940] coded y,uvDC,uvAC intra: 20.2% 2.7% 2.3% inter: 1.4% 0.5% 0.4%\n",
      "[libx264 @ 0x55b05e36d940] i16 v,h,dc,p: 83%  4% 13%  0%\n",
      "[libx264 @ 0x55b05e36d940] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 44%  9% 47%  0%  0%  0%  0%  0%  0%\n",
      "[libx264 @ 0x55b05e36d940] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 41% 30%  8%  2%  6%  3%  3%  3%  3%\n",
      "[libx264 @ 0x55b05e36d940] i8c dc,h,v,p: 98%  1%  1%  0%\n",
      "[libx264 @ 0x55b05e36d940] Weighted P-Frames: Y:0.0% UV:0.0%\n",
      "[libx264 @ 0x55b05e36d940] ref P L0: 51.7%  7.5% 21.1% 19.7%\n",
      "[libx264 @ 0x55b05e36d940] ref B L0: 77.8% 15.2%  7.0%\n",
      "[libx264 @ 0x55b05e36d940] ref B L1: 93.0%  7.0%\n",
      "[libx264 @ 0x55b05e36d940] kb/s:245.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_input (1, 6989, 156)\n",
      "prediction.shape (1, 6989, 102)\n",
      "full_prediction (6989, 102)\n",
      "81600\n",
      "limit 800\n",
      "[[[ 6.64970726e-02  1.44645646e-01  1.09606991e+00]\n",
      "  [ 4.73620743e-03  1.08050868e-01  1.11665711e+00]\n",
      "  [ 1.12585939e-01  7.46731013e-02  1.09251008e+00]\n",
      "  ...\n",
      "  [ 8.57814401e-03  1.01595968e-01  9.52531612e-01]\n",
      "  [-1.58674598e-01  5.84007502e-02  1.09456382e+00]\n",
      "  [ 7.23238736e-02  2.53025889e-01  7.80236280e-01]]\n",
      "\n",
      " [[ 5.26834950e-02  1.71314448e-01  1.08458707e+00]\n",
      "  [-6.92925602e-03  1.35885656e-01  1.11036346e+00]\n",
      "  [ 9.99843925e-02  1.04730412e-01  1.09779129e+00]\n",
      "  ...\n",
      "  [-6.90653920e-03  1.11187860e-01  9.64657283e-01]\n",
      "  [-1.67858273e-01  6.10936619e-02  1.10961053e+00]\n",
      "  [ 6.03713654e-02  2.47765511e-01  7.66304588e-01]]\n",
      "\n",
      " [[ 7.02223182e-02  1.38029113e-01  1.09087596e+00]\n",
      "  [ 7.52680004e-03  9.94205996e-02  1.11175761e+00]\n",
      "  [ 1.18992642e-01  6.86630309e-02  1.09567831e+00]\n",
      "  ...\n",
      "  [ 1.96898729e-02  8.99086595e-02  9.61378670e-01]\n",
      "  [-1.33494571e-01  3.29402536e-02  1.07089398e+00]\n",
      "  [ 1.25710562e-01  2.73005664e-01  7.90072298e-01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 5.62675297e-02  1.26237586e-01  1.09343687e+00]\n",
      "  [-4.92429733e-03  8.84810388e-02  1.10390410e+00]\n",
      "  [ 1.06828034e-01  6.01767451e-02  1.10098085e+00]\n",
      "  ...\n",
      "  [ 1.50127634e-02  9.03756917e-02  9.53722811e-01]\n",
      "  [-1.01166323e-01  8.21171254e-02  1.06180141e+00]\n",
      "  [ 1.34274870e-01  2.16076940e-01  7.25490665e-01]]\n",
      "\n",
      " [[ 6.07203990e-02  1.27455309e-01  1.09249029e+00]\n",
      "  [-6.21423125e-04  8.96928757e-02  1.10311279e+00]\n",
      "  [ 1.11036137e-01  6.06727451e-02  1.10029111e+00]\n",
      "  ...\n",
      "  [ 1.81790739e-02  9.16277468e-02  9.51914704e-01]\n",
      "  [-9.63198319e-02  8.75825807e-02  1.05879209e+00]\n",
      "  [ 1.40882239e-01  2.18505070e-01  7.20225787e-01]]\n",
      "\n",
      " [[ 6.53495640e-02  1.26029283e-01  1.09168521e+00]\n",
      "  [ 3.53771448e-03  8.83016735e-02  1.10326395e+00]\n",
      "  [ 1.14718273e-01  5.81929088e-02  1.09852991e+00]\n",
      "  ...\n",
      "  [ 2.05952823e-02  9.03425217e-02  9.52030218e-01]\n",
      "  [-9.18658376e-02  9.17417333e-02  1.06213132e+00]\n",
      "  [ 1.44351140e-01  2.21846849e-01  7.22676671e-01]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fluidsynth: panic: An error occurred while reading from stdin.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FluidSynth runtime version 2.1.1\n",
      "Copyright (C) 2000-2020 Peter Hanappe and others.\n",
      "Distributed under the LGPL license.\n",
      "SoundFont(R) is a registered trademark of E-mu Systems, Inc.\n",
      "\n",
      "Rendering audio to file './outputvs1-3sic.wav'..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.2.2 Copyright (c) 2000-2019 the FFmpeg developers\n",
      "  built with gcc 7.3.0 (crosstool-NG 1.23.0.449-a04d0)\n",
      "  configuration: --prefix=/home/ilc/anaconda3/envs/sinica --cc=/tmp/build/80754af9/ffmpeg_1587154242452/_build_env/bin/x86_64-conda_cos6-linux-gnu-cc --disable-doc --enable-avresample --enable-gmp --enable-hardcoded-tables --enable-libfreetype --enable-libvpx --enable-pthreads --enable-libopus --enable-postproc --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame --disable-nonfree --enable-gpl --enable-gnutls --disable-openssl --enable-libopenh264 --enable-libx264\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'new_temp_vs1-3sic.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf58.29.100\n",
      "  Duration: 00:00:20.00, start: 0.000000, bitrate: 239 kb/s\n",
      "    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 640x480, 235 kb/s, 40 fps, 40 tbr, 10240 tbn, 80 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "Guessed Channel Layout for Input Stream #1.0 : stereo\n",
      "Input #1, wav, from './outputvs1-3sic.wav':\n",
      "  Duration: 00:02:54.71, bitrate: 1411 kb/s\n",
      "    Stream #1:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, stereo, s16, 1411 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 (h264) -> concat:in0:v0\n",
      "  Stream #1:0 (pcm_s16le) -> concat:in0:a0\n",
      "  concat:out:a0 -> Stream #0:0 (aac)\n",
      "  concat:out:v0 -> Stream #0:1 (libx264)\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x55dd1e278180] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "[libx264 @ 0x55dd1e278180] profile High, level 3.1, 4:2:0, 8-bit\n",
      "[libx264 @ 0x55dd1e278180] 264 - core 157 - H.264/MPEG-4 AVC codec - Copyleft 2003-2018 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=15 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to './video_vs1-3sic_test_predict.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf58.29.100\n",
      "    Stream #0:0: Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s (default)\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 aac\n",
      "    Stream #0:1: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 640x480, q=-1--1, 40 fps, 10240 tbn, 40 tbc (default)\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
      "frame=  800 fps=439 q=-1.0 Lsize=    3325kB time=00:02:54.73 bitrate= 155.9kbits/s speed=95.9x    \n",
      "video:539kB audio:2739kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 1.440983%\n",
      "[aac @ 0x55dd1e276a40] Qavg: 180.446\n",
      "[libx264 @ 0x55dd1e278180] frame I:4     Avg QP:16.30  size: 13683\n",
      "[libx264 @ 0x55dd1e278180] frame P:219   Avg QP:23.52  size:  1275\n",
      "[libx264 @ 0x55dd1e278180] frame B:577   Avg QP:29.01  size:   376\n",
      "[libx264 @ 0x55dd1e278180] consecutive B-frames:  1.8%  4.8%  4.5% 89.0%\n",
      "[libx264 @ 0x55dd1e278180] mb I  I16..4: 26.8% 48.4% 24.8%\n",
      "[libx264 @ 0x55dd1e278180] mb P  I16..4:  0.0%  0.0%  0.0%  P16..4:  1.0%  1.8%  2.2%  0.0%  0.0%    skip:95.0%\n",
      "[libx264 @ 0x55dd1e278180] mb B  I16..4:  0.0%  0.0%  0.0%  B16..8:  1.7%  1.7%  0.9%  direct: 0.3%  skip:95.4%  L0:47.3% L1:40.7% BI:11.9%\n",
      "[libx264 @ 0x55dd1e278180] 8x8 transform intra:48.1% inter:24.5%\n",
      "[libx264 @ 0x55dd1e278180] coded y,uvDC,uvAC intra: 19.5% 2.2% 1.8% inter: 1.2% 0.4% 0.4%\n",
      "[libx264 @ 0x55dd1e278180] i16 v,h,dc,p: 81%  4% 14%  0%\n",
      "[libx264 @ 0x55dd1e278180] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 45%  8% 46%  0%  0%  0%  0%  0%  0%\n",
      "[libx264 @ 0x55dd1e278180] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 41% 31%  7%  2%  6%  3%  4%  3%  3%\n",
      "[libx264 @ 0x55dd1e278180] i8c dc,h,v,p: 98%  1%  1%  0%\n",
      "[libx264 @ 0x55dd1e278180] Weighted P-Frames: Y:0.0% UV:0.0%\n",
      "[libx264 @ 0x55dd1e278180] ref P L0: 52.8%  7.8% 21.5% 17.9%\n",
      "[libx264 @ 0x55dd1e278180] ref B L0: 78.6% 14.2%  7.2%\n",
      "[libx264 @ 0x55dd1e278180] ref B L1: 93.2%  6.8%\n",
      "[libx264 @ 0x55dd1e278180] kb/s:220.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_input (1, 11534, 156)\n",
      "prediction.shape (1, 11534, 102)\n",
      "full_prediction (11534, 102)\n",
      "81600\n",
      "limit 800\n",
      "[[[ 0.09275538  0.10923153  1.09746257]\n",
      "  [ 0.01912919  0.07353182  1.10775874]\n",
      "  [ 0.13199309  0.03939848  1.08087347]\n",
      "  ...\n",
      "  [ 0.02764072  0.08211461  0.94495521]\n",
      "  [-0.03914905  0.16792393  1.07998822]\n",
      "  [ 0.09403341  0.17051506  0.67577926]]\n",
      "\n",
      " [[ 0.0659423   0.1855974   1.08663604]\n",
      "  [ 0.00332499  0.15101495  1.11131892]\n",
      "  [ 0.1080711   0.12085122  1.08242402]\n",
      "  ...\n",
      "  [-0.01543661  0.1252639   0.97322849]\n",
      "  [-0.09703211  0.20841785  1.11903915]\n",
      "  [ 0.01574616  0.19986212  0.67343185]]\n",
      "\n",
      " [[ 0.04467207  0.19257402  1.09122226]\n",
      "  [-0.01911042  0.16006075  1.11668108]\n",
      "  [ 0.08896825  0.12563969  1.09408472]\n",
      "  ...\n",
      "  [-0.03348367  0.12808569  0.98153636]\n",
      "  [-0.1299573   0.19854002  1.13870749]\n",
      "  [-0.0048928   0.20966193  0.70726821]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.07422368  0.12432808  1.0934864 ]\n",
      "  [ 0.01115414  0.08570659  1.10973487]\n",
      "  [ 0.1239897   0.0569272   1.09129939]\n",
      "  ...\n",
      "  [ 0.02848584  0.08193861  0.95859251]\n",
      "  [-0.07248665  0.09269987  1.06078181]\n",
      "  [ 0.14797899  0.22776049  0.6940479 ]]\n",
      "\n",
      " [[ 0.06981172  0.1259663   1.09354923]\n",
      "  [ 0.00657062  0.08874023  1.10823069]\n",
      "  [ 0.11907691  0.05874929  1.09346405]\n",
      "  ...\n",
      "  [ 0.02513169  0.08363277  0.95879838]\n",
      "  [-0.08427612  0.08795282  1.06577442]\n",
      "  [ 0.14022291  0.2230829   0.70014731]]\n",
      "\n",
      " [[ 0.06996866  0.12261649  1.0931196 ]\n",
      "  [ 0.00700638  0.08519614  1.10697792]\n",
      "  [ 0.11944346  0.05576752  1.09322069]\n",
      "  ...\n",
      "  [ 0.02618448  0.08232252  0.95747361]\n",
      "  [-0.08472342  0.08477905  1.06720517]\n",
      "  [ 0.14005788  0.21969384  0.70326117]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fluidsynth: panic: An error occurred while reading from stdin.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FluidSynth runtime version 2.1.1\n",
      "Copyright (C) 2000-2020 Peter Hanappe and others.\n",
      "Distributed under the LGPL license.\n",
      "SoundFont(R) is a registered trademark of E-mu Systems, Inc.\n",
      "\n",
      "Rendering audio to file './outputvs1-2fug.wav'..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.2.2 Copyright (c) 2000-2019 the FFmpeg developers\n",
      "  built with gcc 7.3.0 (crosstool-NG 1.23.0.449-a04d0)\n",
      "  configuration: --prefix=/home/ilc/anaconda3/envs/sinica --cc=/tmp/build/80754af9/ffmpeg_1587154242452/_build_env/bin/x86_64-conda_cos6-linux-gnu-cc --disable-doc --enable-avresample --enable-gmp --enable-hardcoded-tables --enable-libfreetype --enable-libvpx --enable-pthreads --enable-libopus --enable-postproc --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame --disable-nonfree --enable-gpl --enable-gnutls --disable-openssl --enable-libopenh264 --enable-libx264\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'new_temp_vs1-2fug.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf58.29.100\n",
      "  Duration: 00:00:20.00, start: 0.000000, bitrate: 263 kb/s\n",
      "    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 640x480, 259 kb/s, 40 fps, 40 tbr, 10240 tbn, 80 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "Guessed Channel Layout for Input Stream #1.0 : stereo\n",
      "Input #1, wav, from './outputvs1-2fug.wav':\n",
      "  Duration: 00:04:48.34, bitrate: 1411 kb/s\n",
      "    Stream #1:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, stereo, s16, 1411 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 (h264) -> concat:in0:v0\n",
      "  Stream #1:0 (pcm_s16le) -> concat:in0:a0\n",
      "  concat:out:a0 -> Stream #0:0 (aac)\n",
      "  concat:out:v0 -> Stream #0:1 (libx264)\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x562f24026680] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "[libx264 @ 0x562f24026680] profile High, level 3.1, 4:2:0, 8-bit\n",
      "[libx264 @ 0x562f24026680] 264 - core 157 - H.264/MPEG-4 AVC codec - Copyleft 2003-2018 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=15 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to './video_vs1-2fug_test_predict.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf58.29.100\n",
      "    Stream #0:0: Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s (default)\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 aac\n",
      "    Stream #0:1: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 640x480, q=-1--1, 40 fps, 10240 tbn, 40 tbc (default)\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
      "frame=  800 fps=322 q=-1.0 Lsize=    5178kB time=00:04:48.34 bitrate= 147.1kbits/s speed= 116x    \n",
      "video:594kB audio:4518kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 1.296097%\n",
      "[aac @ 0x562f24024f40] Qavg: 260.041\n",
      "[libx264 @ 0x562f24026680] frame I:4     Avg QP:16.13  size: 13444\n",
      "[libx264 @ 0x562f24026680] frame P:231   Avg QP:23.76  size:  1390\n",
      "[libx264 @ 0x562f24026680] frame B:565   Avg QP:29.04  size:   413\n",
      "[libx264 @ 0x562f24026680] consecutive B-frames:  3.1%  5.5%  7.9% 83.5%\n",
      "[libx264 @ 0x562f24026680] mb I  I16..4: 28.5% 46.8% 24.7%\n",
      "[libx264 @ 0x562f24026680] mb P  I16..4:  0.0%  0.0%  0.1%  P16..4:  1.0%  1.8%  2.2%  0.0%  0.0%    skip:94.8%\n",
      "[libx264 @ 0x562f24026680] mb B  I16..4:  0.0%  0.0%  0.0%  B16..8:  1.7%  1.8%  1.0%  direct: 0.3%  skip:95.3%  L0:45.5% L1:41.7% BI:12.8%\n",
      "[libx264 @ 0x562f24026680] 8x8 transform intra:47.1% inter:24.3%\n",
      "[libx264 @ 0x562f24026680] coded y,uvDC,uvAC intra: 20.5% 3.2% 2.8% inter: 1.3% 0.5% 0.4%\n",
      "[libx264 @ 0x562f24026680] i16 v,h,dc,p: 83%  5% 13%  0%\n",
      "[libx264 @ 0x562f24026680] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 44%  5% 50%  0%  0%  0%  0%  0%  0%\n",
      "[libx264 @ 0x562f24026680] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 40% 30%  8%  2%  5%  4%  4%  4%  3%\n",
      "[libx264 @ 0x562f24026680] i8c dc,h,v,p: 98%  1%  1%  0%\n",
      "[libx264 @ 0x562f24026680] Weighted P-Frames: Y:0.0% UV:0.0%\n",
      "[libx264 @ 0x562f24026680] ref P L0: 55.2%  8.3% 20.0% 16.4%\n",
      "[libx264 @ 0x562f24026680] ref B L0: 80.7% 14.1%  5.1%\n",
      "[libx264 @ 0x562f24026680] ref B L1: 93.3%  6.7%\n",
      "[libx264 @ 0x562f24026680] kb/s:243.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_input (1, 7898, 156)\n",
      "prediction.shape (1, 7898, 102)\n",
      "full_prediction (7898, 102)\n",
      "81600\n",
      "limit 800\n",
      "[[[ 7.01931790e-02  1.32361948e-01  1.09280787e+00]\n",
      "  [ 9.21661872e-03  9.09868926e-02  1.10980246e+00]\n",
      "  [ 1.21582948e-01  6.35351539e-02  1.08697019e+00]\n",
      "  ...\n",
      "  [ 1.42150410e-02  9.25019681e-02  9.60306919e-01]\n",
      "  [-1.20856658e-01  9.49248821e-02  1.13092325e+00]\n",
      "  [ 6.23735599e-02  2.04532966e-01  7.23854339e-01]]\n",
      "\n",
      " [[ 7.60195032e-02  1.33963510e-01  1.08706585e+00]\n",
      "  [ 1.49858594e-02  9.52953398e-02  1.10783968e+00]\n",
      "  [ 1.21314958e-01  6.52025118e-02  1.08245358e+00]\n",
      "  ...\n",
      "  [ 1.89691558e-02  8.89421552e-02  9.56016457e-01]\n",
      "  [-1.01252452e-01  8.61087814e-02  1.08845983e+00]\n",
      "  [ 1.01630740e-01  2.07988411e-01  7.07730925e-01]]\n",
      "\n",
      " [[ 7.77217001e-02  1.22461744e-01  1.08942018e+00]\n",
      "  [ 1.42224971e-02  8.61178562e-02  1.11023972e+00]\n",
      "  [ 1.24471441e-01  5.32897040e-02  1.09019098e+00]\n",
      "  ...\n",
      "  [ 2.62598656e-02  8.37194100e-02  9.55978072e-01]\n",
      "  [-9.33095664e-02  8.91763419e-02  1.08924410e+00]\n",
      "  [ 1.17463835e-01  2.02875406e-01  7.18816197e-01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 6.14240393e-02  1.21854901e-01  1.08711252e+00]\n",
      "  [-4.68010828e-03  8.54001492e-02  1.10743711e+00]\n",
      "  [ 1.06718272e-01  5.02931923e-02  1.09311745e+00]\n",
      "  ...\n",
      "  [ 1.14694759e-02  8.09159726e-02  9.61869335e-01]\n",
      "  [-8.80053565e-02  1.01246119e-01  1.08980129e+00]\n",
      "  [ 1.23190172e-01  2.00488895e-01  7.13664746e-01]]\n",
      "\n",
      " [[ 6.35941178e-02  1.19325094e-01  1.08736179e+00]\n",
      "  [-2.87844986e-03  8.27277601e-02  1.10768018e+00]\n",
      "  [ 1.08264983e-01  4.68560383e-02  1.09200010e+00]\n",
      "  ...\n",
      "  [ 1.29788183e-02  7.92832896e-02  9.61988783e-01]\n",
      "  [-8.28368589e-02  1.03965394e-01  1.08976588e+00]\n",
      "  [ 1.26799986e-01  1.99305743e-01  7.07851803e-01]]\n",
      "\n",
      " [[ 6.50610328e-02  1.17515922e-01  1.08889172e+00]\n",
      "  [-8.68771225e-04  8.01141858e-02  1.10983775e+00]\n",
      "  [ 1.10582747e-01  4.52532768e-02  1.09161005e+00]\n",
      "  ...\n",
      "  [ 1.40468627e-02  7.70727098e-02  9.62650931e-01]\n",
      "  [-8.21314380e-02  9.68490541e-02  1.08724174e+00]\n",
      "  [ 1.28429666e-01  2.06680059e-01  7.08328819e-01]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fluidsynth: panic: An error occurred while reading from stdin.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FluidSynth runtime version 2.1.1\n",
      "Copyright (C) 2000-2020 Peter Hanappe and others.\n",
      "Distributed under the LGPL license.\n",
      "SoundFont(R) is a registered trademark of E-mu Systems, Inc.\n",
      "\n",
      "Rendering audio to file './outputvs1-4prs.wav'..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.2.2 Copyright (c) 2000-2019 the FFmpeg developers\n",
      "  built with gcc 7.3.0 (crosstool-NG 1.23.0.449-a04d0)\n",
      "  configuration: --prefix=/home/ilc/anaconda3/envs/sinica --cc=/tmp/build/80754af9/ffmpeg_1587154242452/_build_env/bin/x86_64-conda_cos6-linux-gnu-cc --disable-doc --enable-avresample --enable-gmp --enable-hardcoded-tables --enable-libfreetype --enable-libvpx --enable-pthreads --enable-libopus --enable-postproc --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame --disable-nonfree --enable-gpl --enable-gnutls --disable-openssl --enable-libopenh264 --enable-libx264\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'new_temp_vs1-4prs.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf58.29.100\n",
      "  Duration: 00:00:20.00, start: 0.000000, bitrate: 252 kb/s\n",
      "    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 640x480, 248 kb/s, 40 fps, 40 tbr, 10240 tbn, 80 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "Guessed Channel Layout for Input Stream #1.0 : stereo\n",
      "Input #1, wav, from './outputvs1-4prs.wav':\n",
      "  Duration: 00:03:17.43, bitrate: 1411 kb/s\n",
      "    Stream #1:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, stereo, s16, 1411 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 (h264) -> concat:in0:v0\n",
      "  Stream #1:0 (pcm_s16le) -> concat:in0:a0\n",
      "  concat:out:a0 -> Stream #0:0 (aac)\n",
      "  concat:out:v0 -> Stream #0:1 (libx264)\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x55fd1671aa00] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "[libx264 @ 0x55fd1671aa00] profile High, level 3.1, 4:2:0, 8-bit\n",
      "[libx264 @ 0x55fd1671aa00] 264 - core 157 - H.264/MPEG-4 AVC codec - Copyleft 2003-2018 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=15 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to './video_vs1-4prs_test_predict.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf58.29.100\n",
      "    Stream #0:0: Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s (default)\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 aac\n",
      "    Stream #0:1: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 640x480, q=-1--1, 40 fps, 10240 tbn, 40 tbc (default)\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
      "frame=  800 fps=413 q=-1.0 Lsize=    3713kB time=00:03:17.43 bitrate= 154.1kbits/s speed= 102x    \n",
      "video:567kB audio:3095kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 1.392089%\n",
      "[aac @ 0x55fd167192c0] Qavg: 188.133\n",
      "[libx264 @ 0x55fd1671aa00] frame I:4     Avg QP:16.75  size: 13634\n",
      "[libx264 @ 0x55fd1671aa00] frame P:223   Avg QP:23.68  size:  1325\n",
      "[libx264 @ 0x55fd1671aa00] frame B:573   Avg QP:28.95  size:   401\n",
      "[libx264 @ 0x55fd1671aa00] consecutive B-frames:  2.1%  4.8%  7.1% 86.0%\n",
      "[libx264 @ 0x55fd1671aa00] mb I  I16..4: 27.5% 47.7% 24.8%\n",
      "[libx264 @ 0x55fd1671aa00] mb P  I16..4:  0.0%  0.0%  0.0%  P16..4:  1.0%  1.8%  2.1%  0.0%  0.0%    skip:94.9%\n",
      "[libx264 @ 0x55fd1671aa00] mb B  I16..4:  0.0%  0.0%  0.0%  B16..8:  1.7%  1.8%  0.9%  direct: 0.3%  skip:95.3%  L0:46.5% L1:41.0% BI:12.5%\n",
      "[libx264 @ 0x55fd1671aa00] 8x8 transform intra:49.1% inter:23.9%\n",
      "[libx264 @ 0x55fd1671aa00] coded y,uvDC,uvAC intra: 19.1% 2.1% 1.9% inter: 1.3% 0.5% 0.4%\n",
      "[libx264 @ 0x55fd1671aa00] i16 v,h,dc,p: 82%  4% 14%  0%\n",
      "[libx264 @ 0x55fd1671aa00] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 41%  7% 51%  0%  0%  0%  0%  0%  0%\n",
      "[libx264 @ 0x55fd1671aa00] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 40% 31%  8%  2%  6%  3%  3%  3%  3%\n",
      "[libx264 @ 0x55fd1671aa00] i8c dc,h,v,p: 98%  1%  1%  0%\n",
      "[libx264 @ 0x55fd1671aa00] Weighted P-Frames: Y:0.0% UV:0.0%\n",
      "[libx264 @ 0x55fd1671aa00] ref P L0: 54.9%  8.1% 19.3% 17.6%\n",
      "[libx264 @ 0x55fd1671aa00] ref B L0: 79.6% 14.5%  5.9%\n",
      "[libx264 @ 0x55fd1671aa00] ref B L1: 93.2%  6.8%\n",
      "[libx264 @ 0x55fd1671aa00] kb/s:231.93\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "full_prediction = pd.DataFrame()\n",
    "num_count = 0\n",
    "# read midi\n",
    "# test_dataloader = get_dataloader(test_datapath, batch_size=1)\n",
    "for test_batch in test_data_list:\n",
    "    with torch.no_grad():\n",
    "        # first_target = torch.zeros(test_batch.shape[0],112)\n",
    "        # print(first_target.shape)\n",
    "        test_input = test_batch[None, :]\n",
    "        # test_target = first_target[None, :]\n",
    "        print(\"test_input\", test_input.shape)\n",
    "        # print(\"test_target\", test_target.shape)\n",
    "        prediction = predict(model, test_input, device)\n",
    "        \n",
    "        # print(prediction.shape)\n",
    "        \n",
    "        prediction  = prediction[:, :, :102]\n",
    "        print(\"prediction.shape\", prediction.shape)\n",
    "        \n",
    "        # full_prediction.append(prediction)\n",
    "        full_prediction = pd.DataFrame(prediction[0])\n",
    "        print(\"full_prediction\", full_prediction.shape)\n",
    "        \n",
    "        # prev_prediction = prediction[0][:-1][None, :]\n",
    "        # print(prev_prediction.shape)\n",
    "        \n",
    "        Row_list_prediction =[]\n",
    "        \n",
    "        filecode = test_music_list[num_count]\n",
    "    \n",
    "        # Iterate over each row\n",
    "        for index, rows in full_prediction.iterrows():\n",
    "            #fill nan\n",
    "            rows = rows.fillna(0)\n",
    "            # Create list for the current row\n",
    "            my_list = rows.values.tolist()\n",
    "            # print(my_list)\n",
    "            \n",
    "            my_list_per3 = [my_list[i:i+3] for i in range(0, len(my_list), 3)]\n",
    "            # append the list to the final list\n",
    "            Row_list_prediction.append(my_list_per3)\n",
    "\n",
    "        # print(len(Row_list_prediction), len(Row_list_prediction[0]),len(Row_list_prediction[0][0]))\n",
    "        plot(test_datapath + test_music_list[num_count] + \".mid\", \"./video_\" + filecode + \"_test_predict.mp4\", Row_list_prediction[:800], None, 40, filecode) #ow_list[0:900]\n",
    "        # print(\"prediction.shape\", prediction.shape)\n",
    "        prediction_arr = np.array(Row_list_prediction)\n",
    "        # formated_motion = prediction_format(full_prediction)\n",
    "        # # # plot(formated_motion)\n",
    "        # audio_path = test_music_list[num_count][0]\n",
    "        # output_path = \"test_output_\" + filecode + \".mp4\"\n",
    "        # plot(formated_motion, audio_path, output_path, None, 10, filecode)\n",
    "        num_count += 1\n",
    "\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "772d4827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8172, 115])\n",
      "test_input (1, 8172, 156)\n",
      "test_target torch.Size([1, 8172, 115])\n",
      "prediction.shape (1, 8172, 102)\n",
      "full_prediction (8172, 102)\n",
      "torch.Size([6989, 115])\n",
      "test_input (1, 6989, 156)\n",
      "test_target torch.Size([1, 6989, 115])\n",
      "prediction.shape (1, 6989, 102)\n",
      "full_prediction (6989, 102)\n",
      "torch.Size([11534, 115])\n",
      "test_input (1, 11534, 156)\n",
      "test_target torch.Size([1, 11534, 115])\n",
      "prediction.shape (1, 11534, 102)\n",
      "full_prediction (11534, 102)\n",
      "torch.Size([7898, 115])\n",
      "test_input (1, 7898, 156)\n",
      "test_target torch.Size([1, 7898, 115])\n",
      "prediction.shape (1, 7898, 102)\n",
      "full_prediction (7898, 102)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "full_prediction = pd.DataFrame()\n",
    "num_count = 0\n",
    "# read midi\n",
    "# test_dataloader = get_dataloader(test_datapath, batch_size=1)\n",
    "for test_batch in test_data_list:\n",
    "    with torch.no_grad():\n",
    "        first_target = torch.zeros(test_batch.shape[0],115)\n",
    "        print(first_target.shape)\n",
    "        test_input = test_batch[None, :]\n",
    "        test_target = first_target[None, :]\n",
    "        print(\"test_input\", test_input.shape)\n",
    "        print(\"test_target\", test_target.shape)\n",
    "        prediction = predict(model, test_input, device)\n",
    "        \n",
    "        # print(prediction.shape)\n",
    "        \n",
    "        prediction  = prediction[:, :, :102]\n",
    "        print(\"prediction.shape\", prediction.shape)\n",
    "        \n",
    "        # full_prediction.append(prediction)\n",
    "        full_prediction = pd.DataFrame(prediction[0])\n",
    "        print(\"full_prediction\", full_prediction.shape)\n",
    "        \n",
    "        # prev_prediction = prediction[0][:-1][None, :]\n",
    "        # print(prev_prediction.shape)\n",
    "        \n",
    "        Row_list_prediction =[]\n",
    "        \n",
    "        filecode = test_music_list[num_count]\n",
    "    \n",
    "        # Iterate over each row\n",
    "        for index, rows in full_prediction.iterrows():\n",
    "            #fill nan\n",
    "            rows = rows.fillna(0)\n",
    "            # Create list for the current row\n",
    "            my_list = rows.values.tolist()\n",
    "            # print(my_list)\n",
    "            \n",
    "            my_list_per3 = [my_list[i:i+3] for i in range(0, len(my_list), 3)]\n",
    "            # append the list to the final list\n",
    "            Row_list_prediction.append(my_list_per3)\n",
    "\n",
    "        prediction_arr = np.array(Row_list_prediction)\n",
    "        if not os.path.exists('./output_prediction/[audio_with_anno]'+str(num_layers)+'LSTM_hidden'+str(hidden_size)+'_'+str(num_epochs)+'epoch/'):\n",
    "            os.makedirs('./output_prediction/[audio_with_anno]'+str(num_layers)+'LSTM_hidden'+str(hidden_size)+'_'+str(num_epochs)+'epoch/')\n",
    "        audio_data_output = open('./output_prediction/[audio_with_anno]'+str(num_layers)+'LSTM_hidden'+str(hidden_size)+'_'+str(num_epochs)+'epoch/prediction_'+\n",
    "                                filecode +'.pkl', 'wb')\n",
    "        pickle.dump(prediction_arr, audio_data_output)\n",
    "        audio_data_output.close()\n",
    "        \n",
    "        num_count += 1\n",
    "\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98bac6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_val_loss = evaluate_lstm(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee221509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(final_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5caff77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
